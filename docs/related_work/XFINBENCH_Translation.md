# XFINBENCH: 复杂金融问题解决与推理中的大语言模型基准测试

> **原文标题**: XFINBENCH: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning
> 
> **作者**: Zhihan Zhang¹, Yixin Cao², Lizi Liao¹
> 
> **机构**: ¹新加坡管理大学计算与信息系统学院, ²复旦大学可信具身人工智能研究院
> 
> **翻译状态**: ✅ 已完成
> 
> **翻译说明**: 本文档是对XFINBENCH论文的完整中文翻译，包含论文所有核心内容、图表分析和实验结果

---

## 📋 翻译进度追踪

- [x] 标题和作者信息
- [x] 摘要 (Abstract)  
- [x] 引言 (Introduction)
- [x] 相关工作 (Related Work)
- [x] 数据集构建 (Dataset Construction)
- [x] 实验设置 (Experimental Setup)
- [x] 主要结果 (Main Results)
- [x] 知识增强方法 (Knowledge Augmentation)
- [x] 错误分析 (Error Analysis)
- [x] 结论 (Conclusion)
- [x] 局限性与致谢 (Limitations & Acknowledgments)

**翻译完成度**: 100% ✅

---

## 🎯 论文概述

**XFINBENCH** 是一个专门用于评估大语言模型在复杂、知识密集型金融问题解决能力的新型基准测试，涵盖多模态上下文环境。该基准包含4,235个样本，识别出LLMs在金融领域的五项核心能力：术语理解、时序推理、未来预测、情境规划和数值建模。

---

## 📚 翻译内容

### 摘要 (Abstract)

解决金融问题需要复杂推理、多模态数据处理和广泛的技术理解，这对当前的大语言模型（LLMs）提出了独特挑战。我们引入了**XFINBENCH**，这是一个包含4,235个样本的新型基准，旨在评估LLM在解决复杂、知识密集型金融问题的能力，涵盖多样化的研究生水平金融主题和多模态上下文。

我们使用XFINBENCH识别出LLMs的五项核心能力：**术语理解**、**时序推理**、**未来预测**、**情境规划**和**数值建模**。基于XFINBENCH，我们对18个领先模型进行了广泛实验。结果显示，o1是表现最佳的纯文本模型，总体准确率为67.3%，但仍显著落后于人类专家12.5%，特别是在时序推理和情境规划能力方面。

我们进一步构建了包含3,032个金融术语的知识库用于知识增强分析，发现与问题相关的知识仅对小型开源模型带来一致的准确性提升。此外，我们的错误分析揭示，**计算过程中的舍入误差**和**对图像中曲线位置和交点的视觉盲点**是导致模型在计算和视觉上下文问题中表现不佳的两个主要问题。

---

### 1. 引言 (Introduction)

金融构成了一个关键领域，其特点是需要复杂的问题解决技能。除了领域特定知识外，它还需要高级能力，如时序推理（Su等，2024；Wang和Zhao，2024）、未来预测（Jin等，2024；Zhou等，2023）、情境规划（Valmeekam等，2022；Geva等，2021）和数值建模（Zhao等，2024；Kedziorski等，2024）。此外，现实世界中的复杂金融问题通常涉及丰富的多模态信息，涵盖时间序列（Yu等，2023）、长表格数据（Reddy等，2024）和各种图表（Masry等，2022；Lu等，2024）。这些复杂性对大语言模型（LLMs）提出了重大挑战，因此金融成为评估LLMs的合适测试平台。

已有众多数据集被策划用于评估AI系统在金融领域的推理能力（见表1）。现有数据集（Zhu等，2021；Chen等，2021；Zhao等，2022）主要专注于从公司财务披露中提取数值信息并执行简单计算。最近的工作致力于评估LLMs在知识密集型金融任务上的性能（Kedziorski等，2024；Zhao等，2024；Zhang等，2023）。然而，这些基准很大程度上忽视了金融数据的多模态特性，未能捕获解决复杂现实金融问题所需的高级推理能力，如时序推理和规划。

为了弥补这一差距，我们引入了**XFINBENCH**，这是一个专门设计用于评估LLM在解决复杂、知识密集型金融问题能力的新型基准，涵盖多样化金融主题和多模态上下文。XFINBENCH包含4,235个来自研究生水平金融教科书的样本，确保了我们数据集中金融问题的复杂性，并为每个问题的真实知识标注带来便利。

与仅评估模型对专门金融词汇掌握程度（即术语理解）的现有数据集不同，XFINBENCH识别出复杂金融问题解决的四个更高级能力（§A和图2）：
1. **时序推理**：涉及对基于时间的数据和时间关系的理解（§A.2.2）
2. **未来预测**：基于理论金融模型预测金融趋势的逻辑推理测试（§A.2.3）
3. **情境规划**：分析不同潜在未来情境以评估其对金融决策和策略影响（§A.2.4）
4. **数值建模**：涉及构建公司和产品财务表现的结构化表示（§A.2.5）

此外，XFINBENCH包含三项任务：**陈述判断**（评估模型对金融概念的理解）、**多选题问答**（评估视觉数据的战略决策和预测能力）和**金融计算**（测试金融数学推理）。为了进一步研究领域特定知识如何提升LLM在复杂金融问题上的表现，我们还开发了包含3,032个金融术语的知识库，通过人工标注与金融问题整合。

#### 图1：XFINBENCH五项能力评估结果雷达图

**主要模型在五项核心能力上的表现对比：**

根据雷达图显示的评估结果：

1. **术语理解 (Terminology Understanding)**：
   - 人类专家：~90分（最高）
   - 主要LLMs表现：80-85分范围
   - 模型与人类差距相对较小

2. **时序推理 (Temporal Reasoning)**：
   - 人类专家：~85分
   - o1：~65分
   - 其他模型：50-60分范围
   - 显著的人机差距

3. **未来预测 (Future Forecasting)**：
   - 人类专家：~80分
   - 主要LLMs：60-70分范围
   - 中等程度的人机差距

4. **情境规划 (Scenario Planning)**：
   - 人类专家：~90分
   - LLMs表现：40-60分范围
   - **最大的人机差距**

5. **数值建模 (Numerical Modelling)**：
   - 人类专家：~85分
   - LLMs表现：65-75分范围
   - 中等人机差距

**关键发现**：
- **o1模型**在大多数能力上表现最佳（绿线）
- **claude-3.5-sonnet**在视觉上下文任务中表现突出（蓝线）
- **情境规划**是LLMs最薄弱的环节
- **术语理解**是LLMs相对最接近人类的能力

*注：o1和Llama-3.1-405B的结果不包含视觉上下文问题*

---

基于XFINBENCH，我们对18个领先模型进行了广泛实验，实施思维链（CoT）和思维程序（PoT）方法，并建立了人类专家基线。实验结果表明，XFINBENCH代表了一个严格且具有挑战性的基准，为推进LLMs在复杂金融问题解决和推理方面的发展提供了重要评估工具。

### 主要贡献

我们的贡献总结如下：

• **新型基准**：我们提出了XFINBENCH，一个专门设计用于评估LLM在多模态上下文中解决复杂、知识密集型金融问题能力的新型基准（§3）。

• **全面评估**：我们对18个领先LLMs进行了广泛实验，并将其与人类专家在复杂金融问题解决的五项基本能力上的表现进行比较（§4.2）。

• **知识增强策略**：我们设计了三种知识增强的检索策略（§4.3），并识别了挑战性金融任务中的多种错误类型（§4.4）。

---

### 2. 相关工作 (Related Work)

在金融领域，已经开发了广泛的数据集来评估AI系统的推理能力，如表1所示。现有的金融数据集，包括TAT-QA（Zhu等，2021）、FinQA（Chen等，2021）、MultiHiertt（Zhao等，2022）、PACIFIC（Deng等，2022）和ConvFinQA（Chen等，2022），主要专注于从公司财务报告中进行数量提取和基本数值推理任务。然而，它们缺乏需要广泛金融知识或复杂推理过程的问题。

最近的基准转向知识密集型任务。例如，BizBench（Kedziorski等，2024）从金融证书考试和现有数据集中收集样本来测试LLMs的商业和金融理解能力；FinanceMATH（Zhao等，2024）强调LLMs在金融领域内的数学推理和代码完成能力；FinEval（Zhang等，2023）专注于模型对中文金融概念的理解。尽管如此，这些基准仍未能捕捉解决复杂金融问题所必需的高级能力，如时序推理、预测和规划。

#### 表1：XFINBENCH与现有数据集的比较

| 数据集 | 规模 | 任务类型 | 模态 | 知识密集型 | 数学推理 | 复杂问题 | 数据来源 |
|---------|------|---------|------|----------|--------|--------|----------|
| TAT-QA | 16,552 | 数量提取 | 表格 | ✗ | ✓ | ✗ | 财务报告 + 众包 |
| PACIFIC | 2,757 | 数量提取 | 表格 | ✗ | ✓ | ✗ | 现有数据集 + 自动流水线 |
| FinQA | 8,281 | 数量提取 | 表格 | ✗ | ✓ | ✗ | 财务报告 + 众包 |
| ConvFinQA | 3,892 | 数量提取 | 表格 | ✗ | ✓ | ✗ | 现有数据集 + 众包 |
| FinEval | 4,661 | 多选题问答 | 无 | ✓ | ✗ | ✗ | 中文教科书 |
| BizBench | 19,842 | 数量提取<br/>多选题问答 | 表格 | ✓ | ✓ | ✗ | 现有数据集，证书考试 |
| FinanceMATH | 1,259 | 金融计算 | 表格 | ✓ | ✓ | 部分 | 互联网 + 众包 |
| **XFINBENCH** | **4,235** | **陈述判断<br/>多选题问答<br/>金融计算** | **表格，图像** | **✓** | **✓** | **✓** | **教科书 + 众包 + GPT-4o** |

**关键对比亮点**：

1. **独特的多模态支持**：XFINBENCH是唯一同时支持表格和图像模态的基准
2. **全面的任务类型**：包括三种不同类型的任务
3. **真正的复杂问题**：唯一完全支持复杂金融问题的基准
4. **知识密集型设计**：结合高质量教科书和专家标注

#### 多模态数据集的局限性

现有的覆盖金融领域的多模态数据集主要评估模型的视觉识别能力，忽略了从金融图表中获取有意义见解的领域特定推理（§A.3）。MMMU（Yue等，2024）、MMLU-Pro（Wang等，2024a）和MathVista（Lu等，2024）等基准包括基于图表的问题，但它们通常专注于描述性任务，如识别数值或趋势和识别技术术语。此外，ChartQA（Masry等，2022）、MMC（Liu等，2024）和CharXiv（Wang等，2024b）等面向图表的基准强调通用视觉识别和推理，同时忽略了视觉数据的上下文金融解释。

相比之下，**XFINBENCH引入了领域特定的视角**，要求模型将视觉理解与金融推理相结合，能够对现实金融场景中的AI能力进行更全面的评估。

---

### 3. 数据集构建 (Dataset Construction)

我们的基准XFINBENCH是精心设计的，旨在促进知识密集型金融任务中的复杂推理。数据集构建始于从三本研究生水平金融教科书及其解题手册中收集问题和答案，同时创建金融术语知识库。为了丰富数据集，人类专家为每个问答对标注相关的金融术语和相关能力。由于教科书中的开放式问题对评估提出了挑战，我们在“生成-然后-验证”框架内利用GPT-4o扩展数据集，并提高其评估LLMs的适用性。最后，由人类专家进行的严格质量验证过程确保数据集满足最高的准确性和相关性标准。

#### 3.1 初始数据收集 (Initial Data Collection)

**初始问答数据集的收集**

为了确保我们基准的复杂性和知识密集型特性，我们从三本著名的研究生水平金融教科书中提取课后练习题，这些教科书涵盖了大部分金融主题：

1. **《公司金融基础》** (Fundamentals of Corporate Finance)
2. **《期权期货及其他衣生品》** (Options Futures and Other Derivatives)
3. **《货币银行和金融市场经济学》** (The Economics of Money Banking and Financial Markets)

这些教科书及其解题手册来自互联网上的公开平台，严格遵守版权和许可法规。我们通过pdfplumber库的OCR技术从下载PDF中提取文本。三名标注者被分配在每章末尾收集课后练习题，并为任何伴随的视觉或表格上下文截取屏幕截图。表格数据随后使用GPT-4o格式化为LaTeX格式。

**数据收集统计**：
- **总计**：从教科书中编辑了2,018道课后练习题
- **多模态**：其中343道问题包含视觉或表格上下文

#### 任务分类

然后我们将从教科书收集的课后练习题分类为三个任务：**陈述判断**、**多选题问答**和**金融计算**。

- **陈述判断任务**：评估对金融概念和理论模型基本理解的问题
- **多选题问答任务**：专注于金融策略和模型应用的问题（某些问题可能同时属于这两个任务）
- **金融计算任务**：涉及数值推理的问题

**任务分布统计**：
- **陈述判断**：813道题目
- **多选题问答**：624道题目  
- **金融计算**：858道题目（详见§B.2）

#### 知识库收集 (Collection of Knowledge Bank)

我们构建了一个金融术语及其定义的知识库，以便在评估期间进行知识增强分析。利用每本教科书末尾的主题索引，我们识别其中所有的金融术语及其对应的页码范围。然后标注者从指定页面手动提取这些术语的定义。值得注意的是，一些术语共享相同的页面，导致定义共享。

**知识库统计**：
- **术语总数**：3,032个金融术语
- **独特定义**：1,766个独特定义（详见§B.3）

#### 问答与知识库桥接 (Bridging QA and Knowledge Bank)

到目前为止，我们已经收集了每本教科书中的课后问答对和金融术语，它们最初通过章节联系。在每一章中，主体部分介绍了一组金融术语，然后在章末提出课后问题。我们的标注者被指示为每道课后问题标注来自同一章主体内容的1-3个最相关的金融术语。

**标注统计**：
- **平均关联**：每个问题平均标注1.3个术语（详见§B.3）

#### 3.2 GPT-4o增强标注 (GPT-4o Enhanced Annotation)

教科书中的课后问题大多是开放式的或包含一系列子问题，这使得评估模型的响应变得困难。例如：

- **开放式问题**："讨论期权和远期合约的优缺点"的答案包括期权和期货合约的属性列表
- **计算问题**："一项投资提供...如果付款持续15年，其价值是多少？40年呢？永远呢？"包含一系列具有不同最终答案的子问题

为确保XFINBENCH中的每个问题都能被准确和便捷地评估，我们在"生成-然后-验证"框架下利用GPT-4o进一步处理这些课后问题（Zhang等，2024）。

**生成阶段 (Generation Stage)**

我们采用少样本提示指导GPT-4o将开放式问题转换为具有明确最终答案的问题。

- **陈述判断任务**：要求GPT-4o从每道课后练习题中提取真实和错误陈述。为确保真假陈述之间的平衡，我们应用两个提示模板，使用相同的课后问题作为少样本示例，但一个包含真实陈述，另一个包含错误陈述（见§G.2.1）。

- **多选题问答任务**：遵循STARC规则（Berzak等，2020）指导GPT-4o重新表述每道课后问题，并生成三个候选选项：一个有证据支持的正确答案和两个看似合理但具有误导性的干扰项（见§G.2.2）。

- **金融计算任务**：要求GPT-4o将复杂的课后问题分解为一系列具有明确最终答案的独立问题（见§G.2.3）。

**生成统计**：生成阶段从课后问题中总共生成了6,227道题目。

**验证阶段 (Verification Stage)**

我们从多个维度验证生成阶段问题的质量。主要评估生成问题和答案的**正确性**和**完整性**。具体来说，我们评估：

1. 问题是否提供了获得最终答案的完整背景信息
2. 给定课后问题和其黄金答案，最终答案对问题是否正确

此外，为确保陈述判断任务中问题的独立性，我们验证在同一课后问题内，真实陈述不会为错误陈述的错误性提供证据支持。对于多选题问答任务，我们验证两个误导性选择是否与正确选择互斥，但在措辞和长度上相似。对于金融计算任务，我们验证最终答案是否为纯数值而不包含任何文本。

**验证统计**：验证阶段丢弃了35.2%的问题（详见§B.2）。

#### 图2：XFINBENCH数据集示例

**三个典型任务示例详解**：

**示例1：多选题问答（情境规划能力）**
- **问题**：假设人们对未来房地产价格的预期突然增加。在给定的债券市场供需图中，原始平衡点O会移动到以下哪个点？
- **答案**：Point B
- **术语**：债券市场供需 (Supply & Demand in Bond Market)
- **能力**：未来预测 (Future Forecasting)
- **特点**：需要理解宏观经济变化对债券市场的影响

**示例2：金融计算（时序推理能力）**
- **问题**：Great Pumpkin Farms刚刚支付了每股3.50美元的股息。股息增长率预期永久保持在每年5%。投资者在前三年要求16%的股票回报率，接下来三年要求14%，此后要求11%。当前股价是多少？
- **答案**：50.75美元
- **术语**：非恒定增长股息模型 (Nonconstant-growth dividend model)
- **能力**：时序推理 (Temporal Reasoning)
- **特点**：涉及复杂的多阶段贴现现金流计算

**示例3：金融计算（情境规划能力）**
- **问题**：一个3个月的美式看涨期权，执行价格为20美元。股价为20美元，无风险利率为每年3%，波动率为每年25%。预期1.5个月后有2美元股息。使用三步二项树模型计算期权价格是多少？
- **答案**：0.674美元
- **术语**：无股息股票上的美式期权 (American options on non-dividend stock)
- **能力**：情境规划 (Scenario Planning)
- **特点**：需要构建二项树模型并考虑股息影响

**图2关键特征**：
- **多模态展示**：结合文字、公式、图表和计算树
- **能力映射**：每个示例明确对应特定的认知能力
- **复杂性递增**：从概念理解到数值计算的难度梯度
- **实际应用**：反映真实金融从业者面临的问题类型

#### 3.3 人工质量验证 (Human Quality Validation)

我们实施了全面的验证协议，以确保XFINBENCH中所有标注样本的高质量。对于每个样本，我们分配三名评估者验证以下方面：

1. **问题流畅性**：问题是否流畅并包含获得最终答案的完整信息
2. **答案正确性**：根据课后问题的黄金答案，最终答案是否正确
3. **知识有用性**：标注的金融术语是否有助于回答问题

每个标准在1-5量表上单独评分。值得注意的是，我们的评估者可以访问相应的课后问题及黄金答案和知识库，这与后续实验部分中人类表现的闭卷设置不同。

**质量验证结果**（平均评分S ≥ 4的样本比例）：
- **问题流畅性**：97.1%
- **问题完整性**：96.8%
- **答案正确性**：98.0%
- **知识有用性**：91.2%

这些结果说明了XFINBENCH的高质量（详见表6和§B.4的详细结果）。

#### 3.4 数据统计 (Data Statistics)

表2总结了XFINBENCH的关键统计信息，包含4,235个样本，分为验证集（1,000个样本）和测试集（3,235个样本）。划分基于课后问题的随机抽样。验证集支持模型开发验证，而测试集保留用于标准评估，其答案不会公开发布以防止数据污染。

#### 表2：XFINBENCH关键统计

**XFINBENCH数据集**
| 统计项目 | 数量/比例 |
|---------|----------|
| 总题目数 | 4,235 |
| - 陈述判断 | 1,795 (42.4%) |
| - 多选题问答 | 761 (18.0%) |
| - 含图像 | 146 |
| - 金融计算 | 1,679 (39.6%) |
| - 含表格 | 330 |
| 问题长度（中位数/平均数） | 244 / 273.7 字符 |
| 每题术语数（中位数/平均数） | 1.0 / 1.3 |
| 测试集大小 | 3,235 |
| 验证集大小 | 1,000 |

**知识库**
| 统计项目 | 数量/比例 |
|---------|----------|
| 总术语数 | 3,032 |
| 独特定义数 | 1,766 |
| - 含数学公式 | 34.3% |
| 定义长度（中位数/平均数） | 830 / 1,249 字符 |

#### 图3：XFINBENCH中金融主题分布

**三大核心金融领域**：

1. **期权和衡生品** (Option and Derivatives)：**39.9%**
   - 涵盖期权定价、衡生品策略、风险管理
   - 包含复杂的数学建模和情境分析

2. **公司金融** (Corporate Finance)：**37.1%**
   - 包括资本结构、投资决策、企业估值
   - 涉及现金流分析和财务规划

3. **宏观金融市场** (Macro Financial Market)：**23.0%**
   - 涵盖货币政策、市场动态、经济指标
   - 包含宏观经济分析和市场预测

*注：占总数不足4.5%的主题为清晰起见已省略*

**分布特点**：
- **均衡覆盖**：三大领域相对均衡，确保全面性
- **实用导向**：重点关注实际金融从业中的核心领域
- **复杂性层次**：从基础概念到高级应用的完整谱系

XFINBENCH涵盖28个金融主题，知识库包含3,032个金融术语和1,766个独特定义。数据集在三大核心领域间保持均衡分布，确保了评估的全面性和实用性。

---

### 4. 实验 (Experiments)

我们进行了定性和定量研究，以全面评估领先LLMs在知识密集型金融任务中的表现。

#### 4.1 实验设置 (Experimental Setup)

我们在XFINBENCH测试集上评估模型，采用两种设置：

**1) 多模态大语言模型 (MLLMs)** - 允许视觉输入：
- **gpt-4o** (OpenAI, 2024b)
- **gpt-4o-mini** (OpenAI, 2024a) 
- **claude-3.5-sonnet** (Anthropic, 2024b)
- **claude-3-opus**, **claude-3-haiku** (Anthropic, 2024a)
- **gemini-1.5-flash**, **gemini-1.5-pro** (Team, 2024b)
- **Llama-3.2-Vision模型** (Meta, 2024c)

**2) 纯文本大语言模型** - 仅允许文本输入：
- **o1** (OpenAI, 2024c)
- **o1-mini** (OpenAI, 2024d)
- **deepseek-chat** (DeepSeek-AI, 2024)
- **Llama-3.1模型** (Meta, 2024a)
- **Llama-3模型** (Meta, 2024b) 
- **Mixtral-7×8B** (Jiang等, 2024)

所有MLLMs都允许纯文本输入，除了Llama-3.2-Vision模型，我们在纯文本任务中为其提供空白图像。

**评估方法**：
- 采用**思维链(CoT)方法** (Wei等, 2022)，使用准确率(Acc)评估三项任务
- 在金融计算任务中，额外应用**思维程序(PoT)方法** (Chen等, 2023)
- 使用**AccERR@5**评估，测量在正确答案0.5%误差范围内的准确率

**人类基线**：我们与三名研究生水平的人类专家建立了人类性能基线，在XFINBENCH测试集的随机1,000样本子集上进行闭卷测试(§D.2)。他们均未参与数据集构建。

#### 表3：XFINBENCH上的模型性能

**模型性能完整对比表**：

| 模型 | 陈述判断 | 多选题 | 金融计算(CoT) | 金融计算(PoT) | 含图像 | 含表格 | 全部 | 总体 |
|------|---------|-----------|---------------|---------------|--------|--------|------|------|
| **多模态大语言模型** |
| gpt-4o | 84.0 | 91.5 | 65.3 | 30.1/47.0 | 37.9/59.2 | 25.4/44.2 | 33.2/55.2 | **63.6** |
| gpt-4o-mini | 76.5 | 86.8 | 54.8 | 25.4/38.4 | 30.3/48.4 | 17.6/37.8 | 25.3/49.5 | 57.4 |
| **claude-3.5-sonnet** | **84.3** | **94.2** | **63.7** | **29.8/46.9** | **37.9/59.6** | **34.2/47.5** | **42.2/54.5** | **64.1** |
| claude-3-opus | 79.0 | 91.2 | 50.7 | 25.6/39.5 | 35.7/55.2 | 27.9/38.3 | 40.1/52.0 | 59.7 |
| claude-3-haiku | 70.0 | 82.9 | 43.6 | 15.1/22.5 | 23.8/33.9 | 23.3/28.8 | 34.7/40.4 | 50.1 |
| gemini-1.5-flash | 74.0 | 82.5 | 49.2 | 22.4/30.7 | 28.9/40.1 | 16.4/37.7 | 23.8/48.0 | 54.5 |
| gemini-1.5-pro | 76.3 | 86.5 | 50.8 | 25.0/37.4 | 32.5/44.0 | 24.9/40.7 | 32.9/50.5 | 57.3 |
| Llama-3.2-90B-Vision | 57.4 | 70.9 | 47.6 | 14.4/19.9 | 18.1/20.6 | 11.3/21.4 | 13.2/25.9 | 42.0 |
| Llama-3.2-11B-Vision | 51.8 | 70.3 | 42.0 | 8.3/12.3 | 11.2/12.6 | 8.2/15.9 | 14.4/16.0 | 36.9 |
| **纯文本大语言模型** |
| **o1** | **87.6** | **94.0** | N/A | **34.2/62.0** | N/A | **42.2/66.4** | **30.5/51.4** | **67.3** |
| o1-mini | 81.0 | 90.0 | N/A | 29.7/52.1 | N/A | 39.0/60.6 | 28.9/48.2 | 62.0 |
| Llama-3.1-405B | 83.6 | 91.9 | N/A | 26.2/39.6 | N/A | 34.7/48.7 | 14.1/28.4 | 61.9 |
| deepseek-chat | 74.4 | 88.2 | N/A | 29.2/44.6 | N/A | 37.9/55.2 | 21.8/45.9 | 59.6 |
| Llama-3.1-70B | 80.5 | 90.0 | N/A | 24.1/35.6 | N/A | 31.4/43.0 | 11.0/26.1 | 59.3 |
| Llama-3-70B | 78.2 | 85.9 | N/A | 19.9/27.9 | N/A | 28.5/38.3 | 7.2/18.5 | 56.1 |
| Llama-3.1-8B | 65.3 | 77.8 | N/A | 11.6/16.8 | N/A | 17.0/24.5 | 10.3/18.8 | 45.5 |
| Llama-3-8B | 63.0 | 75.9 | N/A | 8.3/12.6 | N/A | 14.4/19.1 | 7.0/12.8 | 42.9 |
| Mixtral-8×7B | 26.1 | 29.9 | N/A | 1.4/1.7 | N/A | 2.5/4.3 | 1.4/0.6 | 16.6 |
| **人类专家** | **90.9** | **92.1** | **81.1** | **63.8/77.6** | **74.6/83.6** | **79.8** | N/A | N/A |

*注：输入格式 - Q:问题, I:图像, T:表格。"a/b"格式中，a指精确匹配准确率，b指AccERR@5。深红色表示各模型组最高分，浅红色表示第二高分。*

#### 4.2 主要结果 (Main Results)

**多模态大语言模型表现**：
- **claude-3.5-sonnet**取得最佳性能，在XFINBENCH上达到**64.1%准确率**
- **gpt-4o**紧随其后，总体准确率为**63.6%**，在视觉上下文问题中表现最佳，达到**65.3%**

**纯文本大语言模型表现**：
- **o1**在XFINBENCH几乎所有任务中都达到最高准确率，总体准确率为**67.3%**
- 然而，仍比人类表现低**12.5%**，突出表明我们的基准仍有显著改进空间
- 大参数开源模型**Llama-3.1-405B**达到与o1-mini相当的性能，在纯文本任务中甚至超过gpt-4o-mini
- 但大多数开源模型表现不佳，归因于缺乏领域知识和数学推理能力

**PoT方法分析**：
我们观察到PoT（思维程序）提示方法在金融计算任务中恶化了大多数模型的性能。为了更好地分析这些不同性能结果的原因，我们检查了模型在XFINBENCH上使用PoT提示的执行率，测量生成的Python程序中有多少是可执行的。图4(a)显示了不同模型在执行率和AccERR@5准确率之间的关系，表明应用PoT提示时性能下降归因于低执行率。

例如，虽然Llama-3.1-405B使用CoT提示取得了竞争性表现，但它难以持续生成可执行的Python解决方案，导致使用PoT提示时准确率较低。有趣的是，虽然o1的执行率落后于大多数闭源模型，但它在AccERR@5上实现了最高准确率分数，见证了其在复杂任务上的强大而高效的推理能力。

**关键发现**：
1. **人机差距显著**：最佳模型仍落后人类专家12.5%
2. **任务复杂性递增**：从陈述判断→多选题→金融计算，准确率明显下降
3. **视觉理解挑战**：含图像和表格的任务对所有模型都具挑战性
4. **规模效应明显**：大参数模型在金融领域表现明显更好
5. **PoT方法挑战**：大多数模型在PoT设置下由于低执行率而表现下降

#### 4.3 知识增强方法 (Knowledge Augmentation Method)

我们探索了使用外部知识库增强模型的性能，并应用两种检索器从知识库中获取与问题相关的top-n知识术语，即**BM25**和**Ada Embed**，其中n设置为3。回顾我们为每个问题标注了最相关的金融术语，我们进一步设计了一个**Oracle设置**，在该设置中模型被提供真实的金融术语。

#### 图4：PoT方法下准确率与执行率关系及知识增强性能分析

**子图(a)：PoT方法下准确率与执行率关系**
- 显示了不同模型在PoT设置下AccERR@5准确率与执行率的关系
- **关键发现**：执行率低是PoT方法性能下降的主要原因
- **o1模型**：尽管执行率不是最高，但准确率最佳，显示出强大推理能力
- **开源模型挑战**：Llama系列模型在代码生成和执行方面存在明显不足

**子图(b)：三种检索设置下的性能改进**
我们在图4(b)中报告了四个模型在知识库增强下的性能改进情况：

1. **Oracle设置**：在大多数模型上带来最稳健的改进，突出了我们标注数据集的高质量
2. **BM25检索器**：对大多数模型都有性能提升
3. **Ada Embed检索器**：同样对大多数模型带来改进
4. **Llama-3.1-405B例外**：BM25和Ada Embed检索器都导致其性能下降

**子图(c)：Oracle设置下五种能力的性能改进**
我们在图4(c)中报告了Oracle设置下五种金融能力的性能改进：

1. **术语理解 (Terminology Understanding)**：获得最显著改进，所有模型都有明显提升
2. **时序推理 (Temporal Reasoning)**：大多数模型有中等程度改进
3. **未来预测 (Future Forecasting)**：改进有限，GPT-4o甚至出现负面影响
4. **情境规划 (Scenario Planning)**：改进程度中等，但变化较大
5. **数值建模 (Numerical Modelling)**：最小的开源模型Llama-3.1-8B在该能力上显示出最大改进

**知识增强关键发现**：
- **Oracle设置优势**：提供真实相关术语时，模型性能改进最明显
- **小模型受益更多**：Llama-3.1-8B在大多数能力上显示出最大改进，特别是在数值建模方面
- **能力差异明显**：术语理解从知识增强中受益最多，而未来预测受益最少
- **大模型知识饱和**：大参数模型可能已具备足够领域知识，外部知识增益有限

#### 4.4 错误分析 (Error Analysis)

我们对金融计算任务、视觉上下文问题和知识增强方法进行了错误分析。人工标注者被指导进行错误类型标注。

#### 金融计算的错误分析

我们从o1在金融计算任务中的响应中随机选择了400个样本，观察到计算任务中错误响应的两个主要原因：

1. **舍入误差 (Rounding Error)**：存在于中间计算步骤中
2. **知识误用 (Knowledge Misuse)**：应用错误或不完整的金融公式进行计算

图5(a)显示55.2%的o1响应具有正确的推理路径，没有中间舍入误差或知识误用。知识误用在错误推理响应中出现更频繁，而舍入误差经常存在于正确推理过程中。

**典型错误案例**：
- **舍入误差示例**：在二项树构建中的舍入误差导致最终答案错误
- **知识误用示例**：o1未能使用美式期权的主要属性（即在到期日前行使期权以获得利润最大化），因此在后续节点中进行了不必要的计算

#### 视觉上下文的错误分析

我们从GPT-4o在视觉上下文多选题回答任务中的响应中随机选择了100个样本，识别出两种主要错误类型：

1. **视觉盲点 (Blindness)**：模型在识别两条曲线的位置和/或交点方面存在困难
2. **知识误用 (Knowledge Misuse)**：引入不相关知识，从而破坏推理路径

图5(b)显示模型响应具有正确推理但存在视觉盲点（24%）或知识误用（3%）。值得注意的是，35%的响应包含视觉盲点，突出表明视觉盲点是生成式基础模型中错误的主要来源。

#### 知识增强的错误分析

我们从GPT-4o在Oracle设置下提供错误最终答案的响应中随机选择了100个样本。当模型被增强真实金融术语但仍未能提供正确最终答案时，识别出三种错误类型：

1. **推理错误 (Reasoning Error)**：出现在模型推理过程中，与增强知识没有直接关系
2. **过度思考 (Over Thinking)**：增强知识提供直接解决方案，但模型进一步推理超出问题范围的步骤
3. **过度依赖 (Over Reliance)**：模型的推理过程完全由增强知识指导，放弃了回答问题的简单方法

#### 图5：错误分析详细结果

**子图(a)：o1在金融计算任务中的错误分析**
- **正确推理无错误**：55.2%的响应
- **正确推理含舍入误差**：21.5%
- **正确推理含知识误用**：3.0%
- **错误推理含舍入误差**：1.2%
- **错误推理含知识误用**：19.2%

**子图(b)：GPT-4o在视觉上下文问题中的错误分析**
- **正确推理无错误**：64%的响应
- **正确推理含视觉盲点**：24%
- **正确推理含知识误用**：3%
- **错误推理含视觉盲点**：11%
- **错误推理含知识误用**：21%

**子图(c)：GPT-4o在Oracle设置下的错误分析**
按能力类型分布：
- **术语理解**：推理错误52.6%，过度思考29.8%，过度依赖17.5%
- **时序推理**：推理错误80.0%，过度依赖20.0%
- **未来预测**：推理错误37.5%，过度思考62.5%
- **情境规划**：推理错误33.3%，过度依赖66.7%
- **数值建模**：推理错误66.7%，过度思考9.5%，过度依赖23.8%

#### 图6：错误分析案例研究

**案例(a)：知识增强前的知识误用**
- **问题**：股价当前为40美元，预期回报率和波动率分别为15%和25%。连续复利下两年期间的预期回报值是多少？
- **GPT-4o错误响应**："预期回报率15%在整个期间保持不变...因此答案是15%"
- **错误类型**：未意识到当前价格和未来价值的统计关系
- **正确答案**：12%

**案例(b)：计算中的舍入误差和知识误用**
- **问题**：三个月美式看涨期权，执行价格20美元，使用三步二项树计算期权价格
- **o1错误响应**：忽略早期行权特性，在树构建中出现舍入误差
- **错误类型**：知识误用（忽略美式期权特性）+ 舍入误差
- **正确答案**：0.674美元

**案例(c)：视觉上下文问题中的视觉盲点和知识误用**
- **问题**：Fed主席宣布明年利率将大幅上升，债券市场供需图中原始平衡点O将移动到哪个点？
- **GPT-4o错误分析**：
  - 理论推理正确：需求下降，供给上升
  - 视觉识别错误：错误识别曲线交点
  - 知识误用：认为供给保持不变
- **正确答案**：Point B

#### 关键错误模式总结

**金融计算任务**：
- 舍入误差是正确推理中的主要问题
- 知识误用在错误推理中更常见
- 复杂计算步骤增加错误风险

**视觉上下文任务**：
- 视觉盲点是最主要的错误来源（35%的响应）
- 图表交点和曲线位置识别困难
- 领域知识与视觉理解结合挑战

**知识增强任务**：
- 推理错误与增强知识关系较小
- 过度思考在未来预测任务中最频繁
- 过度依赖在情境规划任务中最常见
- 不同能力类型展现不同的错误模式

### 5. 结论 (Conclusion)

在这项工作中，我们介绍了**XFINBENCH**，这是一个包含4,235个样本的基准测试，旨在评估LLMs在多样化主题和多模态上下文中解决复杂、知识密集型金融问题的能力。

#### 主要发现

评估结果表明，虽然**o1是表现最佳的纯文本模型，总体准确率为67.3%**，但它仍**显著落后于人类专家12.5%**，特别是在**时序推理**和**情境规划**能力方面。

进一步分析揭示：
- **知识集成的有限效果**：整合真实知识在解决复杂金融问题方面仅产生有限的性能改进
- **关键技术瓶颈**：模型的计算能力局限和视觉信息识别能力不足是金融领域进展的重大障碍

#### 研究意义

这些发现突出了**XFINBENCH在推动AI智能体发展中的关键作用**，使其能够有效解决复杂的多模态金融问题。我们的基准为金融AI的未来发展提供了严格且具有挑战性的评估标准。

### 局限性 (Limitation)

我们的工作评估了大语言模型在多样化主题和多模态上下文中解决复杂金融问题的能力。根据敏感性分析，我们用于评估的提示模板包括三个关键组件：

1. **角色扮演系统消息**
2. **思维链或思维程序推理方法的应用**  
3. **明确的输出要求**

如果生成的响应不符合指定的输出格式，这种方法可能会影响模型性能。

此外，虽然**XFINBENCH以其复杂性和高质量著称**，但相对于专注于简单任务（如数量提取）的金融数据集，它包含的问答对数量相对较少。

### 致谢 (Acknowledgments)

本研究得到新加坡教育部AcRF Tier 2资助（提案ID：T2EP20123-0052）的支持。本材料中表达的任何意见、发现和结论或建议均为作者观点，不代表新加坡教育部的观点。

---

## 📝 翻译注释

- **XFINBENCH**: X代表复杂(compleX)，FIN代表金融(Financial)，BENCH代表基准(Benchmark)
- **Temporal Reasoning**: 时序推理 - 对时间相关数据和关系的理解和推理
- **Future Forecasting**: 未来预测 - 基于已有信息和模型预测未来趋势
- **Scenario Planning**: 情境规划 - 分析多种可能情境的规划和决策能力
- **Numerical Modelling**: 数值建模 - 构建和使用数学模型进行计算分析
- **Chain-of-Thought (CoT)**: 思维链 - 逐步推理的提示方法
- **Program-of-Thought (PoT)**: 思维程序 - 程序化推理的方法
- **BM25**: 经典的基于词频的信息检索算法
- **Ada Embed**: OpenAI的文本嵌入模型，用于语义相似度检索

---

## 🔍 核心概念术语表

**基准测试相关**：
- **AccERR@5**: 允许0.5%误差范围内的准确率评估指标
- **Oracle设置**: 提供真实答案的理想化知识增强设置
- **多模态基准**: 同时支持文本、图像、表格输入的评估基准

**金融能力维度**：
- **术语理解**: 对专业金融概念和词汇的掌握程度
- **时序推理**: 处理时间序列数据和时间依赖关系的能力
- **未来预测**: 基于历史数据和模型预测金融趋势的能力
- **情境规划**: 分析多种假设情况并制定相应策略的能力
- **数值建模**: 构建和使用数学模型进行金融计算的能力

**错误分析类型**：
- **舍入误差**: 计算过程中的数值精度损失
- **知识误用**: 应用错误或不完整的领域知识
- **视觉盲点**: 无法正确识别图表中的关键视觉元素
- **过度思考**: 超出问题范围的不必要推理
- **过度依赖**: 过分依赖提供的外部知识而忽视简单方法
