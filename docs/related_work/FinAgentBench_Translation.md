# FinAgentBench: 金融问答中智能体检索的基准数据集

> **原文标题**: FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering
> 
> **翻译状态**: 完成 ✓
> 
> **翻译说明**: 本文档是对FinAgentBench论文的中文翻译，将逐步添加原文内容的翻译

---

## 📋 翻译进度追踪

- [x] 标题和摘要
- [x] 引言 (Introduction)
- [x] 相关工作 (Related Work)  
- [x] 数据集构建 (Dataset Construction)
- [x] 数据标注 (Data Annotation)
- [x] 实验部分 (Experiments)
- [x] 结果与分析 (Results and Analysis)
- [x] 结论 (Conclusion)
- [x] 完整翻译 (Complete Translation)

---

## 🎯 论文概述

**FinAgentBench** 是一个专门用于评估金融问答中智能体检索能力的基准数据集。

---

## 📚 翻译内容

### 摘要 (Abstract)

准确的信息检索（IR）在金融领域至关重要，投资者必须从大量文档集合中识别相关信息。传统的信息检索方法——无论是稀疏检索还是密集检索——在检索准确性方面往往存在不足，因为这不仅需要捕获语义相似性，还需要对文档结构和领域特定知识进行细粒度推理。

大型语言模型（LLMs）的最新进展为多步推理检索开辟了新的机会，其中模型通过对哪些信息与给定查询最相关进行迭代推理来对段落进行排序。然而，目前还没有基准来评估金融领域的此类能力。

为了解决这一差距，我们引入了**FinAgentBench**，这是第一个用于评估金融领域多步推理检索的大规模基准——我们称这种设置为**智能体检索**（agentic retrieval）。该基准包含针对标普500上市公司的26,000个专家标注样本，评估LLM智能体是否能够：(1) 在候选文档类型中识别最相关的文档类型，以及 (2) 在选定文档中定位关键段落。

我们的评估框架明确分离了这两个推理步骤以解决上下文限制问题。这种设计能够为理解金融领域中以检索为中心的LLM行为提供定量基础。我们评估了一套最先进的模型，并进一步展示了有针对性的微调如何显著提高智能体检索性能。我们的基准为研究复杂、领域特定的金融任务中以检索为中心的LLM行为奠定了基础。

**关键词**：信息检索，生成式检索，大型语言模型

---

### 1. 引言 (Introduction)

信息检索（IR）是一个基础研究领域，研究如何从大规模文本集合中有效搜索和检索相关信息[21, 23]。其在现实世界应用中的实用重要性使其成为自计算机科学早期以来的核心和长期研究领域[15]。信息检索已经从基于词频的稀疏方法[24]（依赖精确的关键词匹配）演进到将文本嵌入到连续潜在空间中以捕获更深层语义的密集神经检索模型[10]。在金融领域，准确的检索至关重要，因为投资者依赖于对大量申报文件和报告的精确访问来做出高风险、时间敏感的决策。为了支持这一需求，已经开发了金融特定的IR基准[6, 8]，能够对这个复杂、数据丰富领域中的稀疏和密集检索器进行严格评估。

然而，最近的研究揭示了稀疏和密集检索方法的持续准确性上限[2, 12]，特别是在需要对复杂文档进行细粒度理解和结构化推理的领域。为了克服这些限制，最近的研究转向了大型语言模型（LLMs），它们具有强大的语言理解能力和处理长上下文的能力[7, 13, 29]。一个有前景的方向是生成式检索，其中LLM生成给定查询和文档集合中最相关文档的索引[16]。这些结果显示了性能的显著提升，表明检索系统正朝着嵌入更深层推理能力的方向转变[1, 18]。

在金融领域，这种转变尤其相关。金融文档通常冗长且密集，经常需要多步推理：首先识别哪种文档类型（例如，10-K、财报电话会议记录）最匹配信息需求，然后在其中定位具体证据。因此，简单的一次性检索往往是不够的——准确的性能取决于反映专业人士实际搜索信息方式的多步推理。尽管这种多步推理具有实用重要性，但目前还没有基准来评估LLMs在这种设置下的能力。这留下了关于LLMs是否能够在金融这样的高风险领域中作为有效检索智能体的关键问题——在这些领域中，精确性、可解释性和结构化导航是必不可少的。

#### 传统检索 vs 智能体检索对比

**图1：传统检索与我们提出的智能体检索的比较**

传统检索系统在单个步骤中进行检索，而智能体检索通过多步推理将任务分解为文档排序和块排序：

**传统检索流程：**
- 输入查询 → 单步检索 → 生成答案

**智能体检索流程：**
1. **文档级排序**：通过LLM智能体的多步推理检索
2. **块级排序**：在单个文档中进行推理
3. **迭代优化**：如果没有信息则迭代
4. **双重评估**：(1) 文档级排序的理想排名评估 (2) 块级排序的理想排名评估

**文档类型覆盖：**
- 10-K（年度报告）
- 10-Q（季度报告）
- 财报电话会议记录（Earnings）
- DEF14A（代理声明）
- 8-K（重大事件报告）

为了解决这一差距，我们引入了第一个大规模基准**FinAgentBench**，用于评估金融领域的生成式检索系统，重点关注我们称为智能体检索的设置。与评估单阶段检索的先前基准不同，我们提出的基准评估LLM智能体通过两阶段流水线推理和检索相关信息的能力：(1) 识别要检索哪个文档，通过文档级排序进行评估，以及 (2) 确定文档的哪个部分需要关注，通过块级排序进行评估。该基准包含约26,000个样本，由真实世界的金融文档与专家编写的查询和标注构建而成，反映了专业投资者面临的真实用例。通过捕捉检索准确性和推理深度，它为系统分析金融等高风险领域中基于LLM的生成式检索的优势和局限性提供了基础。

### 主要贡献

本工作的主要贡献有三个方面：

• **首创基准**：我们提出了FinAgentBench，据我们所知，这是第一个用于评估金融智能体检索的大规模基准，包含约26,000个专家标注样本，涵盖文档级和块级排序任务。

• **全面评估**：我们在我们的基准上评估了一系列最先进的LLMs，揭示了它们在应用于真实世界金融检索场景时的准确性和推理表现。

• **微调效果**：我们进一步研究了在智能体检索任务上微调LLMs的影响，证明了有针对性的监督可以显著提高文档选择和块级推理性能。

---

### 2. 相关工作 (Related Work)

#### 检索系统

信息检索系统已经从早期的稀疏方法（如TF-IDF和BM25 [20, 22]）演进，这些方法依赖精确的词汇重叠，发展到将查询和文档映射到共享语义空间的密集双编码器。Dense Passage Retrieval (DPR) [9] 和 E5-Mistral [25] 等显著进展通过捕获更深层的语义信息改善了开放域问答的性能。尽管取得了这些进展，稀疏和密集检索器在多跳推理和长上下文查询方面仍然存在困难——这突出了固定检索流水线的内在局限性。

为了解决这一问题，生成式检索将任务重新定义为序列生成，使模型能够直接产生相关的文档标识符或内容 [11]。CAG [3] 和 Infinite Retrieval [27] 等最近的方法通过缓存文档来绕过传统索引，利用LLMs的语言建模和注意力能力。同时，ReAct [26] 等智能体推理框架结合了工具使用和逐步推理，表明LLMs具有迭代决定检索什么以及为什么检索的潜力——将检索与规划和决策制定联系起来。

#### 金融检索基准

金融领域的检索面临独特挑战：文档冗长、查询复杂且精确性至关重要。现有数据集如FinQA [4]、TAT-QA [28]和FiQA [14]解决特定的金融推理需求，而FinanceBench [8]和FinDER [6]等更新的语料库支持开放域和检索增强生成任务。然而，这些基准仍然将检索假设为固定的子程序，通常依赖向量搜索或关键词搜索——并且不评估LLMs是否能够推理检索什么或在长文档中查看哪里。

我们提出的**FinAgentBench**填补了这一空白，直接评估金融领域的智能体检索。它评估LLM智能体是否能够 (1) 生成正确的文档标识符，以及 (2) 对该文档内最相关的块进行排序，为高风险环境中端到端推理驱动检索提供严格的测试平台。

#### 图2：FinAgentBench中的智能体检索流水线

**双阶段智能体检索流程详解：**

**第一阶段：文档级排序 (Document-Level Ranking)**
- **输入**：自然语言查询
- **智能体**：文档类型选择智能体 (Document Type Selection Agent)
- **处理过程**：
  - 基于领域知识进行排序
  - 对5种SEC文件类型进行相关性排名：10-K、10-Q、8-K、财报电话会议记录、DEF14A
  - 从最相关到最不相关进行排序
- **输出**：选择最相关的文档类型

**第二阶段：块级排序 (Chunk-Level Ranking in Single Document)**
- **输入**：选定文档中的分块内容 (Chunk a, Chunk b, ..., Chunk k)
- **智能体**：块选择智能体 (Chunk Selection Agent)
- **处理过程**：
  - 基于领域知识对文档内部块进行排序
  - 识别top-k个最相关的段落
  - 从最相关到最不相关进行排序
- **输出**：选定的相关块 (Chunk c, Chunk e, Chunk b, ..., Chunk f)

该流程体现了专家在金融信息检索中采用的顺序推理步骤：先确定文档类型，再定位具体内容。

---

### 3. FinAgentBench数据集

本节描述FinAgentBench的构建过程，这是一个用于评估金融智能体检索的大规模基准。第3.1节概述了智能体检索的需求动机，随后是任务形式化和推理流水线。第3.2节详细说明文档和查询收集过程。第3.3节描述标注协议和数据集统计信息。

#### 3.1 问题设置 (Problem Setup)

为了检索准确信息，金融检索由于数据量大和金融披露的规律性需要多步推理 [5]。当利用大型语言模型的推理能力来提高检索准确性时，金融文档的大量长度和冗余性——即使单个10-K文件也可能超过数百页——构成了重大挑战，使得在没有任何过滤或优先级排序的情况下处理所有内容变得低效 [19]。与强制对所有可用文本进行推理相关的计算成本和延迟使得这种方法不切实际，特别是在处理多个公司和时期的许多文档时。

因此，为了保持效率，系统应该首先选择最可能包含答案的文档类型——这是可行的，因为申报文件遵循可预测的惯例，不同信息按文档类型一致性组织（例如，10-K中的风险因素，财报电话会议中的战略评论）。然后应该识别选定文档内的相关块或段落。这激发了两阶段检索过程——文档选择后跟段落选择——我们称之为智能体检索，因为它反映了专家采用的顺序推理步骤。

图2提供了我们基准测试的智能体检索工作流程概览。在测试时，系统被提供一个自然语言查询q，通常由专业投资者针对特定公司发出，以及包含从2010年到2024年超过35,000份美国企业披露的文档集合D。在FinAgentBench中，我们包括10-K、10-Q、8-K、财报电话会议记录和DEF 14A代理声明，这些是金融检索中至关重要的最常用公开申报文件。检索任务遵循两阶段推理流水线：

**阶段1：文档级排序 (Stage 1: Document-Level Ranking)**

智能体不是搜索整个语料库，而是首先识别最可能包含答案的文档类型。给定类型集合：

```
T = {10-K, 10-Q, 8-K, Earnings, DEF14A}  (1)
```

模型对T产生一个排序。这一阶段评估模型对金融特定报告惯例的理解——例如，风险因素通常出现在10-K中，而股东提案在DEF-14A申报文件中找到。

**阶段2：块级排序 (Stage 2: Chunk-Level Ranking)**

选定的文档（d_t^★ ∈ D）被分割为不重叠的段落 C(d_t^★) = {c_1, ..., c_M}。智能体对这些块进行排序，并返回按索引排列的前k个段落⟨c^(1), ..., c^(k)⟩。

#### 标注 (Annotations)

FinAgentBench中的每个查询都由领域专家生成和标注。在文档类型层面，提供了T中的黄金标签t_G和黄金排序。在块层面， C(d_t^★)中的每个块元素C^(i)被标注为C_G^(i)，带有黄金相关性评分。对于这些相关性评分的标注，我们遵循TREC Eval [17]标准：0（不相关）、1（部分相关）和2（直接相关）。

#### 评估 (Evaluation)

我们使用标准的top-k指标在两个阶段评估检索准确性：MRR、MAP和nDCG。成功需要在文档类型和块层面都取得准确的性能。

#### 3.2 数据收集 (Data Collection)

我们从真实世界的企业申报文件和专家撰写的查询构建了FinAgentBench，以模拟金融领域中的真实检索任务。数据集包含两个主要组成部分：一个美国企业文档的大型语料库和一组按信息需求分类的细粒度专家编写查询。

**文档收集 (Document Collection)**

我们从美国证券交易委员会（SEC）EDGAR数据库收集申报文件，重点关注2023年到2024年间发布的披露信息。我们的选择包括约3,000家美国上市公司。对于每家公司，我们检索金融专业人士经常使用的五种关键文档类型：10-K、10-Q、8-K、财报电话会议记录和DEF-14A代理声明。总计，语料库包含超过15,000份文档。

每个文档都被预处理为段落级块。为了保持语义和上下文完整性，我们将每个表格视为单一单位，并确保整个表格——包括周围的文本上下文——都封装在单一块内。这一策略在金融文档中特别重要，在这些文档中，表格数据经常传达关键的高密度信息。

在本文中，我们通过专门关注标普500指数中列出的公司来精选语料库的子集。这种过滤产生了一个更有针对性的数据集，包含约500份文档，作为本研究的主要分析范围。

#### 图3：FinAgentBench中两个检索任务的示例

**任务示例详解：**

**任务#1 文档级排序 (Document-Level Ranking)**

*示例查询*：“公司如何看待超大规模云服务的竞争动态？”

*任务指令*：
```
按相关性对以下金融文档类型进行排序以回答问题。将您的排序以从最相关到最不相关的索引列表形式提供。

要排序的文档类型：
0. DEF14A
1. 10-K
2. 10-Q  
3. 8-K
4. Earnings

您的回答必须是精确列表格式的索引列表（例如，[4, 2, 1, 0, 3]）
```

*智能体输出*：
- Rank#1: 2 (10-Q)
- Rank#2: 1 (10-K) 
- Rank#3: 4 (Earnings)

*黄金相关性*：
- "0": 4, "1": 2, "2": 3, "3": 1, "4": 0

**任务#2 块级排序 (Chunk-Level Ranking)**

*示例查询*：“供应链中断如何影响内容的生产时间表？”

*任务指令*：
```
识别并排序最相关的10个文本块来回答这个问题（最好的排在最前）。

文本块：
0. # 美国证券交易委员会 华盛顿特区 20549... CEO Bob Chapek和我们才华横溢的管理团队...
1. # 致股东的信...
...

回答格式：[1st_most_relevant_index, 2nd_most_relevant_index, ..., 10th_most_relevant_index]
```

*智能体输出*：
- Rank#1: 27
- Rank#2: 49
- Rank#3: 32

*黄金相关性*：
- "0": 0, "1": 0, "48": 2, "49": 1, "50": 0, "154": 0

**基准特点**：
- 左侧：文档级排序，智能体根据输入查询的5种SEC文档类型相关性进行排序
- 右侧：块级排序，智能体从选定文档中选择并排序最相关的top-k段落
- 每个块都有黄金真实相关性标签：0（不相关）、1（部分相关）、2（直接相关）
- 使用MRR、MAP和nDCG评估预测结果

#### 3.3 数据标注 (Data Annotation)

为了将原始申报文件和专家编写的查询转换为排序标注数据，我们实施了一个与第3.1节描述的检索阶段一致的结构化两阶段标注流水线。

图3展示了每个任务的示例实例和相应的标注程序。

在第一阶段，文档类型排序中，标注者被给予一个查询和一个固定的5种SEC文档类型集合（10-K、10-Q、8-K、财报电话会议记录和DEF-14A）。他们被要求通过拖拽界面重新排列文档类型，按相关性顺序对其进行排序。在第二阶段，标注者检查排名首位的文档类型中从最相关到最不相关的申报文件的所有段落级块，并根据每个段落回答查询的效果为其分配分级相关性评分（0、1或2）。

所有标注都由具有相关行业经验的金融专业人士进行。每个查询都由两个标注者独立标注，分歧通过简短的裁决过程解决。这种双标注者配置和裁决有效地作为交叉验证机制，提高了标签的可靠性和一致性。

---

### 4. 实验 (Experiments)

我们在FinAgentBench上评估了一系列具有推理能力的LLMs，以评估它们在金融领域进行智能体检索的能力。我们的实验重点关注两个子任务：文档类型排序和块级段落选择。我们还研究了领域特定微调对检索性能的影响。

#### 4.1 实验设置 (Experimental Setup)

我们使用零样本提示基准测试了三个商业LLMs——GPT-o3、Claude-Opus-4和Claude-Sonnet-4。对于每个查询，模型被提示完成两个任务：(1) 从5个SEC申报文件类别集合中对文档类型进行排序，以及 (2) 对选定文档中的段落级块进行排序。我们使用 80/20 分割将文档类型排序和块级相关性任务分为训练集和评估集。

为了评估领域适应的影响，我们还在OpenAI提供的强化微调前后评估了GPT-o4-mini。微调数据来自从FinAgentBench训练集中随机抽样的10%数据。

性能使用标准排序指标通过top-5结果进行测量：归一化折扣累积增益（nDCG）、平均准确率均值（MAP）和平均互惠排序（MRR）。对于块级评估，我们报告排名首位段落的指标，根据专家标注的分级相关性评分进行评估。所有模型都在仅检索的设置下运作，不能访问外部工具或检索增强。

#### 实验结果表格

**表1：推理LLMs在文档排序任务上的评估**

| 模型 | nDCG@5 | MAP@5 | MRR@5 |
|------|--------|-------|-------|
| GPT-o3 | 0.770 | 0.829 | 0.875 |
| Claude-Opus-4 | 0.773 | 0.840 | 0.875 |
| **Claude-Sonnet-4** | **0.783** | **0.849** | **0.892** |

**表2：推理LLMs在块排序任务上的评估**

| 模型 | nDCG@5 | MAP@5 | MRR@5 |
|------|--------|-------|-------|
| GPT-o3 | 0.351 | 0.257 | 0.538 |
| **Claude-Opus-4** | **0.418** | **0.307** | **0.568** |
| Claude-Sonnet-4 | 0.419 | 0.296 | 0.567 |

**表3：强化微调对GPT-o4-mini在两个检索任务上的影响**

| 任务 | 未微调 | | | 微调后 | | |
|------|---------|---------|---------|---------|---------|---------|
| | nDCG@5 | MAP@5 | MRR@5 | nDCG@5 | MAP@5 | MRR@5 |
| 文档排序 | 0.758 | 0.826 | 0.872 | **0.808** | **0.865** | **0.933** |
| 块排序 | 0.345 | 0.256 | 0.526 | **0.371** | **0.274** | **0.587** |

#### 实验结果分析

1. **文档级排序表现**：Claude-Sonnet-4在所有指标上表现最佳，显示出对金融文档类型的强大理解能力。

2. **块级排序挑战**：所有模型在块级任务上的性能明显低于文档级任务，表明细粒度段落选择的复杂性。

3. **微调效果**：强化微调显著提高了GPT-o4-mini在两个任务上的性能，尤其是文档排序任务中MRR@5从0.872提升到0.933。

#### 4.2 结果 (Results)

**文档级排序 (Document-Level Ranking)**

表1报告了各种具有推理能力的LLMs在文档类型排序任务上的性能。所有模型都被提示根据查询对五个候选SEC文档类型进行排序。在这些模型中，Claude-Sonnet-4在所有三个指标上都达到了最高性能，nDCG为0.783、MAP为0.849、MRR为0.892。值得注意的是，所有三个模型的表现都远超随机水平，表明通用LLMs对金融报告结构具有强大的先验知识。然而，模型间的差距表明，对微妙类型级线索的建模受益于架构或规模差异。

**块级排序 (Chunk-Level Ranking)**

表2展示了块级段落检索任务的结果，其中要求模型在选定文档中识别最相关内容的top-5排名。性能普遍低于文档类型设置，反映了细粒度检索的复杂性增加。Claude-Sonnet-4和Claude-Opus-4表现相当，Sonnet在nDCG（0.419）方面略领先，而Opus在MAP（0.307）和MRR（0.568）方面领先。这表明，即使对于强大的LLMs来说，块级推理仍然具有挑战性，可能是由于需要在金融披露的凗长上下文中进行知识密集型推理。

**微调的影响 (Impact of Fine-Tuning)**

表3评估了强化微调对GPT-o4-mini在两个检索阶段的影响。微调产生了实质性的提升：在文档类型排序中，nDCG从0.758提高到0.808，MRR从0.872增加到0.933。在块排序中也观察到类似的改善，MRR从0.526上升到0.587。这些结果表明，领域特定的监督显著提高了检索准确性，无论是在选择正确的文档类型还是在文档内定位相关信息方面。这突出了通过任务对齐的训练信号将LLMs适应到金融推理任务的重要性。

---

### 5. 结论 (Conclusion)

我们引入了**FinAgentBench**，这是一个大规模基准，旨在评估大型语言模型在金融这一高风险领域的智能体检索能力。该基准模拟了现实的投资者对多样化企业申报文件集合的查询，要求模型对文档类型和文档内部内容进行推理。

我们的实验表明，最先进的LLMs已经在金融报告结构上表现出强大的先验知识，在文档类型排序上取得了高性能。然而，它们在块级别识别相关内容的能力仍然有限，反映了在长篇且信息密集的文档中进行细粒度推理的固有挑战。

我们进一步证明，通过领域特定监督的强化式微调可以在两个检索阶段都实质性地提高性能，突出了将LLMs与领域专家标注对齐的价值。

**FinAgentBench为研究复杂领域中的端到端检索行为提供了新的基础**。未来的工作可能会探索增强智能体检索性能，以及检索和生成的联合建模以支持投资决策。

---

## 📝 翻译注释

- **Agentic Retrieval**: 智能体检索 - 指由AI智能体主导的信息检索过程
- **Financial Question Answering**: 金融问答 - 针对金融领域的问题回答任务
- **Benchmark Dataset**: 基准数据集 - 用于评估和比较模型性能的标准数据集
- **Generative Retrieval**: 生成式检索 - 通过生成模型进行信息检索的方法
- **Multi-step Reasoning**: 多步推理 - 需要多个推理步骤的复杂认知过程
- **Document-level Ranking**: 文档级排序 - 对整个文档进行相关性排序
- **Chunk-level Ranking**: 块级排序 - 对文档内部的段落或块进行排序
- **10-K/10-Q/8-K**: 美国SEC要求的标准企业披露文件类型
- **DEF14A**: 代理声明文件
- **Earnings Transcript**: 财报电话会议记录

---

## 🔍 相关概念解释

### 核心技术概念
- **POMDP (Partially Observable Markov Decision Process)**: 部分可观测马尔可夫决策过程
- **nDCG (Normalized Discounted Cumulative Gain)**: 归一化折扣累积增益
- **MAP (Mean Average Precision)**: 平均准确率均值
- **MRR (Mean Reciprocal Rank)**: 平均互惠排序
- **TREC Eval**: 信息检索领域的标准评估方法

---

## 🎆 翻译完成总结

本文档已完成对**FinAgentBench**论文的全面中文翻译，包括所有核心技术内容、实验结果和结论。翻译保持了学术严谨性，专业术语准确一致，为中文读者提供了高质量的智能体检索领域研究参考资料。
